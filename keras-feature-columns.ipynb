{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow feature columns - A great idea\n",
    "\n",
    "Tensorflow's [feature columns](https://www.tensorflow.org/guide/feature_columns) are a great idea. Feature columns allow user's to easily transform input to tensorflow's premade models. For example with feature columns you can specify\n",
    "\n",
    "- how a feature should be normalized.\n",
    "- that a feature should be one-hot-encoded\n",
    "- that a feature should be transformed to an embedding\n",
    "\n",
    "In my opinion feature columns provide a key feature I find missing in [keras](https://keras.io/) when working with proprietary data from day to day. Keras has some really nice preprocessing features that are useful for hard things like image preprocessing and time series. However, I find that keras lacks support for some of the workhorse preprocessing functionality found in sklearn (such as one hot encoding and pipelines). Personally the ease of specifying a categorical column as an embedding offers interesting possibilities for many business applications (think of how many times you've one hot encoded IDs or features like zip code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with feature columns\n",
    "\n",
    "While feature columns support both a preprocessing pipeline and a set of basic transformations easily applied to a variety of industry problems I found that, unfortunately, while a great idea, working with feature columns is awkward if you want to do anything slightly outside the box. In my experience this is mainly due to the fact that feature columns only work with [tensorflow estimators](https://www.tensorflow.org/guide/estimators).\n",
    "\n",
    "My first attempt at working with feature columns was to try and connect feature columns to keras models. Why? Because for time series applications at work I would like to have a convenient way to feed a mixture of numeric and categorical values to an LSTM. Feature columns make the first part easy and keras makes defining an LSTM easy. Since tensorflow now houses a keras API I thought this would be straightforward. I was wrong. The key to getting this to work is to convert your keras model to a tensorflow estimator with [tf.keras.estimator.model_to_estimator](https://www.tensorflow.org/api_docs/python/tf/keras/estimator/model_to_estimator). However, actually connecting your feature columns to your keras model is far from trivial requiring more code than seems worth the trouble.\n",
    "\n",
    "Taking the hint from my first stab at using feature columns, my second attempt was to stick with tensorflow estimators and avoid keras altogether. In this case I simply tried to re-implement a simple linear model I implemented for a project at work some time ago using feature columns and tensorflow's [linear regression estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor). In a short period of time I was able to get the model training. However, there was some outliers that the model predicted poorly on. No problem I thought - I already faced this in my initial implementation and knew that using huber loss would likely remedy the issue. However, after speding more time researching how to switch from the default loss function (MSE) to huber loss than I wish I had I concluded that it isn't possible to do so without writing your own custom estimator. But writing your own estimator was a deal breaker for me - for me, the whole appeal of feature columns was having something that worked out of the box.\n",
    "\n",
    "My last comment is that it's worth noting that the web is pretty silent on how to do anything with tensorflow estimators outside of what you can find in the docs. This [stackoverlfow post](https://stackoverflow.com/questions/50766718/changing-loss-function-for-training-built-in-tensorflow-estimator) (accessed 7/17/18) is pretty indicative of the kind of help you'll find on the subject... crickets.\n",
    "\n",
    "Given the outcome of this adventure I was pretty dissapointed that working with feature columns was so cumbersome considering they seem to be such a great idea. However, rather than give up on the idea, I wrote a few `keras` classes that accomplish what feature columns do. Snippets from the implementation are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class FeatureColumn:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.input = keras.layers.Input((1,), name=self.name)\n",
    "\n",
    "    @property\n",
    "    def output(self):\n",
    "        return self.input\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.values\n",
    "\n",
    "\n",
    "class NumericColumn(FeatureColumn):\n",
    "    def __init__(self, *args, normalizer=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.normalizer = normalizer\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.normalizer is not None:\n",
    "            return self.normalizer(X).values\n",
    "        return X.values\n",
    "\n",
    "\n",
    "class EmbeddingColumn(FeatureColumn):\n",
    "    def __init__(self, *args, vocabulary, output_dim, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.vocabulary = vocabulary\n",
    "        self.vocab_map = {v: i for i, v in enumerate(vocabulary)}\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    @property\n",
    "    def output(self):\n",
    "        embedding = keras.layers.Embedding(\n",
    "            input_dim=len(self.vocabulary)+1,  # +1 for OOV\n",
    "            output_dim=self.output_dim,\n",
    "            input_length=1)(self.input)\n",
    "        return keras.layers.Flatten()(embedding)\n",
    "\n",
    "    def transform(self, X):\n",
    "        mapping = lambda x: self.vocab_map.get(x, len(self.vocabulary))\n",
    "        return X.apply(mapping).values\n",
    "\n",
    "\n",
    "class Features:\n",
    "    def __init__(self, *features):\n",
    "        self.features = features\n",
    "\n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return [f.input for f in self.features]\n",
    "\n",
    "    @property\n",
    "    def output(self):\n",
    "        concat = keras.layers.Concatenate(axis=-1)\n",
    "        return concat([f.output for f in self.features])\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [f.transform(X[f.name]) for f in self.features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "feature1 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "feature2 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 10)        110         feature1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 10)        1010        feature2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 10)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 10)           0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "feature3 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 21)           0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 feature3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           1100        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 50)           2550        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            51          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,821\n",
      "Trainable params: 4,821\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.0905\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame({\n",
    "    'feature1': np.random.randint(10, size=100),\n",
    "    'feature2': np.random.randint(100, size=100),\n",
    "    'feature3': np.random.rand(100)\n",
    "})\n",
    "y = np.random.rand(100)\n",
    "\n",
    "\n",
    "def normalize(column):\n",
    "    mean = X[column].mean()\n",
    "    std = X[column].std()\n",
    "    def normalizer(X, mean=mean, std=std):\n",
    "        return (X - std) / mean\n",
    "\n",
    "\n",
    "features = Features(\n",
    "    EmbeddingColumn('feature1', vocabulary=list(range(10)), output_dim=10),\n",
    "    EmbeddingColumn('feature2', vocabulary=list(range(100)), output_dim=10),\n",
    "    NumericColumn('feature3', normalizer=normalize('feature3')),\n",
    ")\n",
    "\n",
    "\n",
    "x = keras.layers.Dense(50, activation='relu')(features.output)\n",
    "x = keras.layers.Dense(50, activation='relu')(x)\n",
    "x = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.models.Model(inputs=features.inputs, outputs=x)\n",
    "model.summary()\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "_ = model.fit(features.transform(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
