{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow feature columns - A great idea\n",
    "\n",
    "`tensorflow's` [feature columns](https://www.tensorflow.org/guide/feature_columns) are a great idea. Feature columns allow user's to easily transform input to `tensorflow's` premade models. For example with feature columns you can specify\n",
    "\n",
    "- how a feature should be normalized.\n",
    "- that a feature should be one-hot-encoded\n",
    "- that a feature should be encoded using hashing\n",
    "- that a feature should be transformed to an embedding\n",
    "\n",
    "In my opinion feature columns provide a key feature I find missing in [keras](https://keras.io/) when working with proprietary data from day to day. `keras` has some really nice preprocessing features that are useful for hard things like image preprocessing and time series. However, I find that `keras` lacks support for some of the workhorse preprocessing functionality found in `sklearn` (such as one hot encoding and pipelines).\n",
    "\n",
    "While it is possible to tie `sklearn` preprocessing in to `keras` models it's not exactly elegant (see [here](https://keras.io/scikit-learn-api/)). Additionally it's not straightforward to make preprocessing steps *a part* of your model - for example embedding columns. It seems that `tensorflow` is on to a great idea here.\n",
    "\n",
    "**Disclaimer**: In this post I'll share my *opinion* of `tensorflow's` feature columns, both good and bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with feature columns\n",
    "\n",
    "While feature columns are a great idea that support both a preprocessing pipeline and a set of basic transformations easily applied to a variety of industry problems, I found that, unfortunately, working with feature columns is awkward if you want to do anything slightly outside the box. In my experience this is mainly due to the fact that feature columns only work with [tensorflow estimators](https://www.tensorflow.org/guide/estimators).\n",
    "\n",
    "My first attempt at working with feature columns was to try and connect feature columns to `keras` models. Why? Because for time series applications at work I would like to have a convenient way to feed a mixture of numeric and categorical values to an LSTM. Feature columns sound like they should make the first part easy and `keras` makes training an LSTM easy. Since `tensorflow` now houses a `keras` API I thought this would be straightforward. I was wrong. The key to getting this to work is to convert your `keras` model to a `tensorflow` estimator with [tf.keras.estimator.model_to_estimator](https://www.tensorflow.org/api_docs/python/tf/keras/estimator/model_to_estimator). However, actually connecting your feature columns to your `keras` model is far from trivial requiring more code than seems worth the trouble.\n",
    "\n",
    "Taking the hint from my first stab at using feature columns, my second attempt was to stick with `tensorflow` estimators and avoid `keras` altogether. In this case I simply tried to re-implement a simple linear model I implemented for a project at work some time ago using feature columns and `tensorflow's` [linear regression estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor). In a short period of time I was able to get the model training. However, there was some outliers that the model predicted poorly on. No problem  - I already faced this in my initial implementation and knew that using huber loss would likely remedy the issue. However, after spending more time researching how to switch from the default loss function (MSE) to huber loss than I wish I had I concluded that it isn't possible to do so without writing your own custom estimator. But writing your own estimator was a deal breaker for me - for me, the whole appeal of feature columns was having something that worked out of the box.\n",
    "\n",
    "My last comment is that it's worth noting that the web is pretty silent on how to do anything with `tensorflow` estimators outside of what you can find in the docs. This [stackoverlfow post](https://stackoverflow.com/questions/50766718/changing-loss-function-for-training-built-in-tensorflow-estimator) (accessed 7/17/18) is pretty indicative of the kind of help you'll find on the subject... nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras feature columns\n",
    "\n",
    "Given the outcome of this adventure I was pretty dissapointed that working with feature columns was so cumbersome considering they seem to be such a great idea. However, rather than give up on the idea, I wrote a few `keras` classes that accomplish what feature columns do. Snippets from the implementation and a toy example are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class FeatureColumn:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.input = keras.layers.Input((1,), name=self.name)\n",
    "\n",
    "    @property\n",
    "    def output(self):\n",
    "        return self.input\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.values\n",
    "\n",
    "\n",
    "class NumericColumn(FeatureColumn):\n",
    "    def __init__(self, *args, normalizer=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.normalizer = normalizer\n",
    "\n",
    "    def fit(self, X):\n",
    "        if self.normalizer is not None:\n",
    "            self.normalizer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.normalizer is not None:\n",
    "            return self.normalizer.transform(X).values\n",
    "        return X.values\n",
    "    \n",
    "    \n",
    "class EmbeddingColumn(FeatureColumn):\n",
    "    def __init__(self, *args, vocab_size, output_dim, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.vocab_map = {v: i for i, v in enumerate(set(X))}\n",
    "        @np.vectorize\n",
    "        def apply_mapping(X):\n",
    "            return self.vocab_map.get(x, self.vocab_size)\n",
    "        self._apply_mapping = apply_mapping\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self._apply_mapping(X)\n",
    "\n",
    "    @property\n",
    "    def output(self):\n",
    "        embedding = keras.layers.Embedding(\n",
    "            input_dim=self.vocab_size+1,  # +1 for OOV\n",
    "            output_dim=self.output_dim,\n",
    "            input_length=1)(self.input)\n",
    "        return keras.layers.Flatten()(embedding)\n",
    "\n",
    "\n",
    "class FeatureSet:\n",
    "    def __init__(self, *features):\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.features = [f.fit(X[f.name]) for f in self.features]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [f.transform(X[f.name]) for f in self.features]\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return [f.input for f in self.features]\n",
    "\n",
    "    @property\n",
    "    def output(self):\n",
    "        concat = keras.layers.Concatenate(axis=-1)\n",
    "        return concat([f.output for f in self.features])\n",
    "\n",
    "\n",
    "class Scaler:\n",
    "    def __init__(self):\n",
    "        self.mean = self.std = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.mean = np.mean(X)\n",
    "        self.std = np.std(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (X - self.mean) / self.std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "feature1 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "feature2 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 10)        110         feature1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 10)        1010        feature2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 10)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 10)           0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "feature3 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 21)           0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 feature3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           1100        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 50)           2550        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            51          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,821\n",
      "Trainable params: 4,821\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 0.0808\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X = pd.DataFrame({\n",
    "    'feature1': np.random.randint(10, size=100),\n",
    "    'feature2': np.random.randint(100, size=100),\n",
    "    'feature3': np.random.rand(100)\n",
    "})\n",
    "y = np.random.rand(100)\n",
    "\n",
    "\n",
    "def normalize(column):\n",
    "    mean = X[column].mean()\n",
    "    std = X[column].std()\n",
    "    def normalizer(X, mean=mean, std=std):\n",
    "        return (X - std) / mean\n",
    "\n",
    "\n",
    "features = FeatureSet(\n",
    "    EmbeddingColumn('feature1', vocab_size=10, output_dim=10),\n",
    "    EmbeddingColumn('feature2', vocab_size=100, output_dim=10),\n",
    "    NumericColumn('feature3', normalizer=Scaler())\n",
    ")\n",
    "\n",
    "\n",
    "x = keras.layers.Dense(50, activation='relu')(features.output)\n",
    "x = keras.layers.Dense(50, activation='relu')(x)\n",
    "x = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.models.Model(inputs=features.inputs, outputs=x)\n",
    "model.summary()\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "_ = model.fit(features.fit_transform(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
